{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LIS DQN 画像入力\n",
    "学習済みネットワークを使わずに画像をQNetworkの入力としている"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# GYM\n",
    "import sys\n",
    "tmp_path = []\n",
    "for it in sys.path:\n",
    "    if not 'gym' in it:\n",
    "        # print(it)\n",
    "        tmp_path.append(it)\n",
    "sys.path = tmp_path\n",
    "sys.path.append('../..')\n",
    "# faster rcnn\n",
    "import sys\n",
    "sys.path.append('../../../../chainer-faster-rcnn')\n",
    "from lib.cpu_nms import cpu_nms as nms\n",
    "from lib.models.faster_rcnn import FasterRCNN\n",
    "import cv2 as cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import copy\n",
    "import os\n",
    "import random\n",
    "import collections\n",
    "import numpy as np\n",
    "import six.moves.cPickle as pickle\n",
    "\n",
    "import gym\n",
    "from PIL import Image\n",
    "\n",
    "import chainer\n",
    "from chainer import cuda, FunctionSet, Variable, optimizers\n",
    "from chainer import links as L\n",
    "import chainer.functions as F\n",
    "from chainer.links import caffe\n",
    "\n",
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "# set display defaults\n",
    "plt.rcParams['figure.figsize'] = (10, 10)        # large images\n",
    "plt.rcParams['image.interpolation'] = 'nearest'  # don't interpolate: show square pixels\n",
    "plt.rcParams['image.cmap'] = 'gray'  # use grayscale output rather than a (potentially misleading) color heatmap\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class QFunction(chainer.Chain):\n",
    "\n",
    "    def __init__(self, n_actions):\n",
    "        initializer = chainer.initializers.HeNormal()\n",
    "        c1 = 32\n",
    "        c2 = 64\n",
    "        c3 = 64\n",
    "        fc_unit = 256*2\n",
    "\n",
    "        super(QFunction, self).__init__(\n",
    "             # the size of the inputs to each layer will be inferred\n",
    "            conv1=L.Convolution2D(4, c1, 8, stride=4, pad=0),\n",
    "            conv2=L.Convolution2D(c1, c2, 4, stride=2, pad=0),\n",
    "            conv3=L.Convolution2D(c2, c2, 4, stride=2, pad=0),\n",
    "            conv4=L.Convolution2D(c2, c3, 3, stride=1, pad=0),\n",
    "            fc1=L.Linear(6400, fc_unit, initialW=initializer),\n",
    "            fc2=L.Linear(fc_unit, n_actions, initialW=initializer),\n",
    "        )\n",
    "\n",
    "    def __call__(self, x):\n",
    "        x = x/255.\n",
    "        h = F.relu(self.conv1(x))\n",
    "        h = F.relu(self.conv2(h))\n",
    "        h = F.relu(self.conv3(h))\n",
    "        h = F.relu(self.conv4(h))\n",
    "        h = F.relu(self.fc1(h))\n",
    "        y = self.fc2(h)        \n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_greedy_action(Q, obs):\n",
    "    xp = Q.xp\n",
    "    obs = xp.expand_dims(xp.asarray(obs, dtype=np.float32), 0)\n",
    "    with chainer.no_backprop_mode():\n",
    "        q = Q(obs).data[0]\n",
    "    return int(xp.argmax(q))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mean_clipped_loss(y, t):\n",
    "    # Add an axis because F.huber_loss only accepts arrays with ndim >= 2\n",
    "    y = F.expand_dims(y, axis=-1)\n",
    "    t = F.expand_dims(t, axis=-1)\n",
    "    return F.sum(F.huber_loss(y, t, 1.0)) / y.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update(Q, target_Q, opt, samples, gamma=0.99, target_type='dqn'): \n",
    "    xp = Q.xp\n",
    "    s = np.ndarray(shape=(minibatch_size, STATE_LENGTH, FRAME_WIDTH, FRAME_HEIGHT), dtype=np.float32)\n",
    "    a = np.asarray([sample[1] for sample in samples], dtype=np.int32)\n",
    "    r = np.asarray([sample[2] for sample in samples], dtype=np.float32)\n",
    "    done = np.asarray([sample[3] for sample in samples], dtype=np.float32)\n",
    "    s_next = np.ndarray(shape=(minibatch_size, STATE_LENGTH, FRAME_WIDTH, FRAME_HEIGHT), dtype=np.float32)\n",
    "\n",
    "    for i in xrange(minibatch_size):\n",
    "        s[i] = samples[i][0]\n",
    "        s_next[i] = samples[i][4]\n",
    "\n",
    "    # to gpu if available\n",
    "    s = xp.asarray(s)\n",
    "    a = xp.asarray(a)\n",
    "    r = xp.asarray(r)\n",
    "    done = xp.asarray(done)\n",
    "    s_next = xp.asarray(s_next)\n",
    "    \n",
    "    # Prediction: Q(s,a)\n",
    "    y = F.select_item(Q(s), a)\n",
    "    # Target: r + gamma * max Q_b (s',b)\n",
    "    with chainer.no_backprop_mode():\n",
    "        if target_type == 'dqn':\n",
    "            t = xp.sign(r) + gamma * (1 - done) * F.max(target_Q(s_next), axis=1)\n",
    "        elif target_type == 'double_dqn':\n",
    "            t = xp.sign(r) + gamma * (1 - done) * F.select_item(\n",
    "                target_Q(s_next), F.argmax(Q(s_next), axis=1))\n",
    "        else:\n",
    "            raise ValueError('Unsupported target_type: {}'.format(target_type))\n",
    "    loss = mean_clipped_loss(y, t)\n",
    "    Q.cleargrads()\n",
    "    loss.backward()\n",
    "    opt.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def meanQvalue(Q, samples): \n",
    "    xp = Q.xp\n",
    "    s = np.ndarray(shape=(minibatch_size, STATE_LENGTH, FRAME_WIDTH, FRAME_HEIGHT), dtype=np.float32)\n",
    "    a = np.asarray([sample[1] for sample in samples], dtype=np.int32)\n",
    "\n",
    "    for i in xrange(minibatch_size):\n",
    "        s[i] = samples[i][0]\n",
    "\n",
    "    # to gpu if available\n",
    "    s = xp.asarray(s)\n",
    "    a = xp.asarray(a)\n",
    "\n",
    "    # Prediction: Q(s,a)\n",
    "    y = F.select_item(Q(s), a)\n",
    "    mean_Q = (F.sum(y)/minibatch_size).data\n",
    "    return mean_Q\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 入力画像をStateへ変換"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "STATE_LENGTH = 4  # Number of most recent frames to produce the input to the network\n",
    "FRAME_WIDTH = 224  # Resized frame width\n",
    "FRAME_HEIGHT = 224  # Resized frame height\n",
    "import numpy as np\n",
    "from skimage.color import rgb2gray\n",
    "from skimage.transform import resize\n",
    "\n",
    "class ObsProcesser:\n",
    "    mean_file = 'ilsvrc_2012_mean.npy'\n",
    "    mean_name, ext = os.path.splitext(mean_file)\n",
    "\n",
    "    def __init__(self):\n",
    "        self.state = None\n",
    "            \n",
    "    def init_state(self, obs):\n",
    "        processed_obs = self._preprocess_observation(obs)\n",
    "        state = [processed_obs for _ in xrange(STATE_LENGTH)]\n",
    "        self.state = np.stack(state, axis=0)\n",
    "        \n",
    "    def obs2state(self, obs):\n",
    "        processed_obs = self._preprocess_observation(obs)\n",
    "        self.state = np.concatenate((self.state[1:, :, :], processed_obs[np.newaxis]), axis=0)\n",
    "        return self.state\n",
    "    \n",
    "    def _preprocess_observation(self, obs):\n",
    "        img = np.array(obs[\"image\"][0])\n",
    "        preprossed = np.asarray(resize(rgb2gray(img), (FRAME_WIDTH, FRAME_HEIGHT))*255, dtype=np.uint8)\n",
    "        return preprossed\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lis_DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Initialize variables\n",
    "depth_image_dim = 224 * 224\n",
    "M = 10000  # number of episodes\n",
    "replay_start_size = 1000  # steps after which we start to update\n",
    "steps_to_decay_epsilon = 100000  # steps to take to decay epsilon\n",
    "min_epsilon = 0.1  # minimum value of epsilon\n",
    "sync_interval = 5000  # interval of target sync\n",
    "evaluation_interval = 100 # 何エピソードごとに評価するか\n",
    "update_inverval = 1\n",
    "minibatch_size = 32  # size of minibatch\n",
    "update_inverval = 1\n",
    "reward_scale = 1  # scale factor for rewards\n",
    "gpu = 0  # gpu id (-1 to use cpu)\n",
    "target_type = 'dqn'  # 'dqn' or 'double_dqn'\n",
    "#NO_OP_MAX = 10 # maximum number of \"do anything\" actions at the start of an episode\n",
    "save_model_inverval = 25000 # interval of save weights\n",
    "save_folder = 'checkpoints'\n",
    "D = collections.deque(maxlen=10 ** 5)  # replay memory: original 10 ** 6\n",
    "\n",
    "Rs = []  # past returns\n",
    "average_Rs = []\n",
    "eval_Rs = [] # evaluation Max reward\n",
    "eval_steps = [] # evaluation Max\n",
    "step = 0  # total steps taken\n",
    "episode = 0\n",
    "\n",
    "networkname = 'image_input'\n",
    "log_file = os.path.join(save_folder,'reward_'+networkname+'.log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# disable stderre\n",
    "import os\n",
    "import sys\n",
    "f = open(os.devnull, 'w')\n",
    "sys.stderr = f\n",
    "# 開始の時間表示\n",
    "from datetime import datetime\n",
    "print(datetime.now().strftime(\"%Y/%m/%d %H:%M:%S\"))\n",
    "# save folder\n",
    "if not os.path.isdir(save_folder):\n",
    "    os.mkdir(save_folder)\n",
    "# log file\n",
    "with open(log_file, 'w') as the_file:\n",
    "    the_file.write('cycle, episode_reward_sum \\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "enable_controller = [0,1,2]\n",
    "n_actions = len(enable_controller)\n",
    "obs_processer = ObsProcesser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Initialize chainer models\n",
    "Q = QFunction(n_actions)\n",
    "if gpu >= 0:\n",
    "    chainer.cuda.get_device(gpu).use()\n",
    "    Q.to_gpu(gpu)\n",
    "target_Q = copy.deepcopy(Q)\n",
    "opt = optimizers.RMSpropGraves(lr=0.00025, alpha=0.95, momentum=0.95, eps=0.0001)\n",
    "opt.setup(Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "env_name = 'Lis-v2'\n",
    "env = gym.make(env_name)\n",
    "obs = env.reset()\n",
    "obs_processer.init_state(obs)\n",
    "state = obs_processer.state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show observation image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.imshow(np.array(obs['image'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.imshow(obs['depth'][0].reshape(224,224))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Evaluation(epsilon = 0.05):\n",
    "    Rs_eval = []  # past returns\n",
    "    for episode in range(3):\n",
    "\n",
    "        obs = env.reset()\n",
    "        obs_processer.init_state(obs)\n",
    "\n",
    "#         for _ in range(random.randint(1, NO_OP_MAX)):\n",
    "#             a = env.action_space.sample()\n",
    "#             obs, _, _, _ = env.step(a)  # Do anything\n",
    "#             obs_processer.obs2state(obs)\n",
    "            \n",
    "        done = False\n",
    "        R = 0.0\n",
    "        state = obs_processer.state   \n",
    "\n",
    "        while not done:\n",
    "            # Select an action\n",
    "            if np.random.rand() < epsilon:\n",
    "                a = env.action_space.sample()\n",
    "            else:\n",
    "                a = get_greedy_action(Q, state)\n",
    "\n",
    "            # Execute an action\n",
    "            new_obs, r, done, _ = env.step(a)\n",
    "\n",
    "            new_state = obs_processer.obs2state(new_obs)\n",
    "            R += r\n",
    "            state = new_state           \n",
    "\n",
    "        print('Evaluation : episode: {} step: {} R:{}'.format(episode, step, R))\n",
    "        Rs_eval.append(R)\n",
    "    return Rs_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Initialize a figure\n",
    "%matplotlib notebook\n",
    "fig, ax = plt.subplots(1,1)\n",
    "\n",
    "while episode < M:\n",
    "    obs = env.reset()\n",
    "    obs_processer.init_state(obs)\n",
    "\n",
    "#     for _ in range(random.randint(1, NO_OP_MAX)):\n",
    "#         a = env.action_space.sample()\n",
    "#         obs, _, _, _ = env.step(a)  # Do anything\n",
    "#         obs_processer.obs2state(obs)\n",
    "    done = False\n",
    "    R = 0.0\n",
    "    state = obs_processer.state   \n",
    "    \n",
    "    while not done:\n",
    "\n",
    "        # Select an action\n",
    "        epsilon = 1.0 if len(D) < replay_start_size else \\\n",
    "            max(min_epsilon, np.interp(\n",
    "                step, [replay_start_size, replay_start_size+steps_to_decay_epsilon], [1.0, min_epsilon]))\n",
    "        if np.random.rand() < epsilon:\n",
    "            a = env.action_space.sample()\n",
    "        else:\n",
    "            a = get_greedy_action(Q, state)\n",
    "\n",
    "        # Execute an action\n",
    "        new_obs, r, done, info = env.step(a)\n",
    "\n",
    "        new_state = obs_processer.obs2state(new_obs)\n",
    "\n",
    "        R += r\n",
    "\n",
    "        # Store a transition\n",
    "        D.append((state, a, r * reward_scale, done, new_state))\n",
    "        state = new_state\n",
    "\n",
    "        # Sample a random minibatch of transitions\n",
    "        if len(D) >= replay_start_size:\n",
    "            if step % update_inverval == 0:\n",
    "                samples = random.sample(D, minibatch_size)\n",
    "                update(Q, target_Q, opt, samples, target_type=target_type)\n",
    "\n",
    "            if step % sync_interval == 0:\n",
    "                mean_Q = meanQvalue(Q, samples)\n",
    "                target_Q = copy.deepcopy(Q)\n",
    "                print('target Q update! mean Q value : {}, epsilon:{}'.format(mean_Q, epsilon))\n",
    "\n",
    "            if step % save_model_inverval == 0:\n",
    "                save_model_filename = \"lis_dqn_{}_{}.h5\".format(env_name, step)\n",
    "                save_model_path = os.path.join(save_folder, save_model_filename)\n",
    "                print('save model : {}'.format(save_model_path))\n",
    "                chainer.serializers.save_hdf5(save_model_path, Q)\n",
    "\n",
    "        step += 1\n",
    "\n",
    "    with open(log_file, 'a') as the_file:\n",
    "        the_file.write(str(episode) + ',' + str(R) + '\\n')\n",
    "\n",
    "    Rs.append(R)\n",
    "    average_R = np.mean(Rs[-100:])\n",
    "    average_Rs.append(average_R)\n",
    "    if episode % 10 is 0:\n",
    "        print('episode: {} step: {} R:{} average_R:{}'.format(\n",
    "              episode, step, R, average_R))\n",
    "\n",
    "    # evaluation\n",
    "    if (episode+1) % evaluation_interval == 0:\n",
    "        R_eval = Evaluation()\n",
    "        eval_Rs.append(max(R_eval))\n",
    "        eval_steps.append(episode)\n",
    "\n",
    "                \n",
    "    ax.clear()\n",
    "    ax.plot(Rs)\n",
    "    ax.plot(average_Rs)\n",
    "    ax.plot(eval_steps, eval_Rs, \"o\")\n",
    "    fig.canvas.draw()\n",
    "    episode += 1\n",
    "    \n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

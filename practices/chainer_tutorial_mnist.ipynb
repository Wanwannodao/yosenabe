{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chainer mnist tutorial\n",
    "自分の理解が深まるように少し改変  \n",
    "http://docs.chainer.org/en/stable/tutorial/basic.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "from __future__ import print_function\n",
    "import argparse\n",
    "import sys; sys.argv=['']; del sys\n",
    "\n",
    "import numpy as np\n",
    "import chainer\n",
    "import chainer.functions as F\n",
    "import chainer.links as L\n",
    "from chainer import training\n",
    "from chainer.training import extensions\n",
    "from chainer import Variable\n",
    "import copy # test iterator copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class MLP(chainer.Chain):\n",
    "    def __init__(self, n_units, n_out):\n",
    "        super(MLP, self).__init__(\n",
    "             # the size of the inputs to each layer will be inferred\n",
    "            l1=L.Linear(784, n_units),  # n_in -> n_units\n",
    "            l2=L.Linear(n_units, n_units),  # n_units -> n_units\n",
    "            l3=L.Linear(n_units, n_out),    # n_units -> n_out\n",
    "            )\n",
    "    def __call__(self, x):\n",
    "        h1 = F.relu(self.l1(x))\n",
    "        h2 = F.relu(self.l2(h1))\n",
    "        y = self.l3(h2)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Classifier(chainer.Chain):\n",
    "    def __init__(self, predictor):\n",
    "        super(Classifier, self).__init__(predictor=predictor)\n",
    "        self.accuracy = None\n",
    "    def __call__(self, x, t):\n",
    "        y = self.predictor(x)\n",
    "        loss = F.softmax_cross_entropy(y, t)\n",
    "        self.accuracy = F.accuracy(y, t)\n",
    "        chainer.report({'loss': loss, 'accuracy': self.accuracy}, self)\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gpu_id = 0 #  use gpu\n",
    "model = Classifier(MLP(1000, 10))\n",
    "\n",
    "if gpu_id >= 0:\n",
    "    chainer.cuda.get_device(gpu_id).use()  # Make a specified GPU current\n",
    "    model.to_gpu()  # Copy the model to the GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup an optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = chainer.optimizers.SGD()\n",
    "# optimizer = chainer.optimizers.MomentumSGD()\n",
    "optimizer.use_cleargrads()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.Classifier at 0x7f444c634d50>"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer.setup(model)\n",
    "optimizer.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load the MNIST dataset\n",
    "train, test = chainer.datasets.get_mnist()\n",
    "train_iter = chainer.iterators.SerialIterator(train, 100)\n",
    "test_iter = chainer.iterators.SerialIterator(test, 100, repeat=False, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run manually\n",
    "dataはnumpy形式　GPUの有無に応じてcupyに変更する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch = train_iter.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'cupy' from '/home/komatsu/anaconda2/lib/python2.7/site-packages/cupy/__init__.pyc'>"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xp = model.xp\n",
    "xp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = xp.asarray([it[0] for it in batch], dtype=np.float32)\n",
    "t = xp.asarray([it[1] for it in batch], dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6, 2, 3, 4, 5, 6, 4, 0, 8, 2, 6, 6, 9, 5, 7, 6, 1, 8, 7, 1, 6, 5, 6,\n",
       "       9, 0, 0, 6, 1, 7, 1, 1, 4, 6, 7, 9, 2, 0, 2, 6, 4, 9, 2, 9, 8, 3, 9,\n",
       "       8, 5, 8, 7, 7, 9, 3, 5, 1, 5, 1, 4, 0, 3, 4, 9, 4, 8, 8, 2, 5, 9, 6,\n",
       "       3, 9, 9, 8, 0, 4, 1, 7, 3, 1, 0, 3, 5, 6, 6, 8, 0, 5, 9, 7, 8, 3, 2,\n",
       "       3, 7, 2, 1, 3, 9, 3, 8], dtype=int32)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cupy.core.core.ndarray"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 784)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.26716429 -0.0498517   0.18256906  0.23069705  0.05639438 -0.10639153\n",
      "   0.12205505  0.0361958  -0.01332716  0.28266028]\n",
      " [ 0.12073089  0.13905387 -0.07146449  0.06680758  0.10860203 -0.01722271\n",
      "   0.03685853 -0.23692617 -0.03443903  0.20013787]\n",
      " [ 0.15522207 -0.04847419  0.05707862  0.2503624   0.06084583 -0.09175757\n",
      "   0.0527268  -0.0893645  -0.07871582 -0.07998484]\n",
      " [ 0.24428926  0.03566321 -0.10973764  0.10700005  0.11170004 -0.00931807\n",
      "   0.09277786 -0.03669683  0.18683572  0.00750732]\n",
      " [ 0.19380745  0.10304111 -0.13998446  0.27420381  0.16632356  0.06229278\n",
      "   0.11802386 -0.212038   -0.02707849 -0.03473621]\n",
      " [ 0.20962606  0.05662105  0.02327721  0.11459622  0.12235951  0.00976416\n",
      "   0.06178858 -0.00801183 -0.05154353  0.03278422]\n",
      " [ 0.12869045 -0.14108269 -0.04610553  0.07982061 -0.09339776 -0.21632953\n",
      "   0.17145656 -0.09841295  0.07178839 -0.03134733]\n",
      " [ 0.3230553  -0.28360224 -0.01819221  0.22198357  0.00400162 -0.2296188\n",
      "   0.16240102 -0.36118239 -0.18633536  0.22606188]\n",
      " [ 0.05218624 -0.03715081 -0.02645493  0.1386539   0.1818693  -0.23462167\n",
      "   0.08023464 -0.09295366  0.00440845  0.09002037]\n",
      " [ 0.2769275  -0.09750299  0.27043411  0.33094835  0.00887596 -0.04389509\n",
      "   0.24910939  0.02172108 -0.04389711  0.16403468]\n",
      " [-0.00368707 -0.03758631  0.07447536  0.09223062  0.20132984 -0.12011408\n",
      "   0.07948022 -0.08835686 -0.23291096 -0.01176769]\n",
      " [ 0.19413452  0.06854053 -0.07805698  0.23189287  0.15687369 -0.3182711\n",
      "   0.18289408 -0.11528751 -0.15489465  0.23123214]\n",
      " [ 0.13445304 -0.13223109 -0.12822852  0.09665766  0.06201489  0.04854149\n",
      "   0.27002069 -0.27722055  0.00079038  0.37203464]\n",
      " [ 0.14566918  0.27561113 -0.11816468  0.28627032  0.23761109 -0.12019321\n",
      "   0.0377219  -0.35618278  0.01635044  0.02081023]\n",
      " [ 0.2140498   0.09226227 -0.09230226  0.356123    0.0786424  -0.02465985\n",
      "   0.1828891  -0.18812288  0.17107205  0.00896785]\n",
      " [ 0.12997073  0.07294828  0.03071864  0.15397099  0.03804913  0.04620292\n",
      "   0.06756309 -0.02289976  0.05231421  0.10451902]\n",
      " [ 0.14741366  0.20037855  0.00662663  0.16234349  0.11222371 -0.04944319\n",
      "   0.01682146 -0.12589304  0.01794493 -0.11065123]\n",
      " [ 0.09407791 -0.05293801 -0.04487003  0.00764283  0.15960124  0.0279768\n",
      "   0.02777423 -0.16143979  0.04078994  0.30827186]\n",
      " [ 0.32327244  0.09138519  0.15939589  0.14885488  0.31687784 -0.12966922\n",
      "   0.30901882 -0.06781146  0.05558802  0.33970487]\n",
      " [ 0.09759717  0.25508323 -0.07120213  0.08380029  0.20538715 -0.11925164\n",
      "   0.13281947  0.06529079 -0.17457543 -0.09423076]\n",
      " [ 0.27009469 -0.08987315  0.01108558  0.09234063  0.14676505 -0.21222687\n",
      "   0.15372305 -0.0731613  -0.10324907  0.1865411 ]\n",
      " [ 0.04917673 -0.06279907  0.0640628   0.10980596  0.1564545  -0.12305979\n",
      "   0.10123495 -0.17088762  0.15809131 -0.03742484]\n",
      " [ 0.19861694  0.05806558 -0.05320442  0.16191612  0.13442113 -0.11348141\n",
      "   0.21073875 -0.10088492  0.00391933  0.15185153]\n",
      " [ 0.15684685 -0.09100039 -0.01735931  0.14191741  0.11577445  0.07255999\n",
      "   0.21359803 -0.21097413 -0.01010299  0.27929243]\n",
      " [ 0.088968   -0.03798386  0.14232114  0.32094863  0.09260511 -0.0425098\n",
      "   0.06885603 -0.09047179  0.08012813 -0.06735419]\n",
      " [ 0.20909032 -0.0877527   0.13199598  0.23867841  0.11502973 -0.11351079\n",
      "   0.21055019 -0.1705322  -0.02981279  0.02946917]\n",
      " [ 0.14350799  0.01019392  0.2251042   0.20375207  0.29239368 -0.17985055\n",
      "   0.12947409  0.13292144 -0.04472987  0.11254454]\n",
      " [ 0.13470033  0.07628354  0.12647846  0.01246901  0.24066266  0.00841095\n",
      "   0.03470233  0.06822955 -0.27142519 -0.05889934]\n",
      " [-0.0081062   0.11722529  0.04403668  0.1862586   0.04091695 -0.00148178\n",
      "   0.11627511 -0.12978134 -0.00858656  0.02873207]\n",
      " [ 0.2432595   0.20717987  0.06992362  0.1071079   0.07477017 -0.11160287\n",
      "   0.15763584 -0.05509545 -0.14945084 -0.10313937]\n",
      " [ 0.17273767  0.20372628  0.07554527  0.06368332  0.2403682   0.01924565\n",
      "   0.01258203  0.1749174  -0.28877103 -0.05595733]\n",
      " [-0.02995926  0.00181813 -0.06722229  0.24260169 -0.01525648 -0.12299218\n",
      "   0.22284837  0.04344046  0.00990786 -0.10047949]\n",
      " [ 0.16113092  0.25829017 -0.11762013 -0.00762849  0.22204022 -0.10877604\n",
      "   0.12738143 -0.14608376  0.1035907  -0.14039449]\n",
      " [ 0.11639611 -0.03263832 -0.04258148  0.05880193  0.01343325  0.01851741\n",
      "   0.0869812  -0.23579267  0.00223606  0.06793333]\n",
      " [ 0.06660111 -0.02635938  0.1094094   0.11678287  0.1641175  -0.01479024\n",
      "   0.19818313 -0.23416571  0.04678905  0.10204858]\n",
      " [ 0.10255574  0.16068551  0.03265798  0.1467101   0.04076283 -0.0566864\n",
      "   0.18695273 -0.02835755 -0.12938806  0.00107964]\n",
      " [ 0.23194078 -0.13671964 -0.00149875  0.11212116  0.19790627  0.02010942\n",
      "   0.13698913 -0.09576093 -0.16758734  0.08771414]\n",
      " [ 0.35458261 -0.08438361 -0.02172329  0.16751321  0.03872548 -0.15142153\n",
      "   0.22382431 -0.02938568 -0.08382828  0.07920203]\n",
      " [ 0.38959551 -0.10948067  0.09726534  0.28352654  0.15801236 -0.13393047\n",
      "   0.19246764 -0.18737774 -0.11722333  0.2204816 ]\n",
      " [ 0.27723399 -0.21470955  0.06414381  0.33828062  0.16669632  0.03080191\n",
      "   0.19050968 -0.1762384   0.00415228  0.12294362]\n",
      " [ 0.04199685  0.1421814  -0.00735553  0.29017401  0.06206335 -0.12494344\n",
      "   0.11905356 -0.23769762  0.01019097 -0.01180764]\n",
      " [ 0.21873255  0.14866477  0.05038144  0.17167883  0.23771442  0.02208373\n",
      "   0.20281978  0.01398931 -0.2438602  -0.03205091]\n",
      " [-0.03451535  0.09421919  0.08529858  0.24938595  0.09586691 -0.15046437\n",
      "   0.26663896 -0.12145539 -0.01086488  0.1323573 ]\n",
      " [ 0.13649708 -0.00755491  0.04176578  0.220956    0.3197242  -0.09241097\n",
      "   0.15962572 -0.1004927  -0.01487314  0.09406536]\n",
      " [ 0.12267072  0.08850952 -0.15870203  0.0714164   0.17911932  0.00211387\n",
      "   0.03915082 -0.12889719 -0.08518885 -0.05627223]\n",
      " [ 0.18593447  0.04181256 -0.01952682  0.10614358  0.07421121 -0.2010269\n",
      "   0.03769358 -0.10812926  0.00834123  0.02595071]\n",
      " [ 0.05875416 -0.01982659 -0.02094362  0.21241747  0.4052085   0.01894411\n",
      "   0.08949872 -0.11056348 -0.18390648  0.20757324]\n",
      " [ 0.08163726 -0.19533217 -0.26145679  0.27325526 -0.02826575 -0.12811702\n",
      "  -0.06681979 -0.33749348 -0.09650201  0.02959842]\n",
      " [ 0.23818885  0.02093347 -0.23985812  0.29536581  0.07377893 -0.05264299\n",
      "   0.17563644 -0.05031181  0.17460737  0.02314893]\n",
      " [ 0.15288465  0.10979729 -0.13748886  0.19197534  0.20840557 -0.0162337\n",
      "   0.30034465 -0.22105373  0.10391355  0.11632092]\n",
      " [ 0.04815661  0.05093538  0.17094487  0.1482895   0.12405615 -0.0137004\n",
      "   0.10778846 -0.04380868 -0.02339006  0.0270086 ]\n",
      " [ 0.30222762 -0.31634745  0.01558925  0.18727444  0.0413932   0.01316581\n",
      "   0.15239121 -0.11742072  0.19207771  0.05383787]\n",
      " [ 0.09813683  0.08495905 -0.11393835  0.12149469  0.10811339 -0.02969672\n",
      "   0.0692187  -0.08647475 -0.01855314 -0.06231228]\n",
      " [ 0.04519732 -0.04942291 -0.03097119  0.03263372  0.17113748 -0.00591729\n",
      "   0.09319224 -0.31655478 -0.02645704 -0.01373712]\n",
      " [ 0.00837273  0.10279956 -0.00634584  0.14793526  0.03356721 -0.03394346\n",
      "   0.11530992 -0.05445231  0.04650692 -0.01592674]\n",
      " [ 0.20467801 -0.14072621 -0.09265598  0.2178244   0.10691885  0.01825103\n",
      "   0.24139152 -0.02153578  0.09780893  0.08665762]\n",
      " [-0.05140487  0.06390699 -0.02452541  0.06615255  0.1123362  -0.0644455\n",
      "   0.06647779 -0.11646028  0.06755092 -0.04577808]\n",
      " [ 0.19906747 -0.14365904  0.24651586  0.31221893  0.25502798 -0.14320436\n",
      "   0.25093842 -0.15790121 -0.03998768  0.07955699]\n",
      " [ 0.26279518 -0.14819334  0.09859836  0.2785418   0.08621945 -0.22792473\n",
      "   0.05926448 -0.32358286 -0.04715616  0.14821891]\n",
      " [ 0.13322271 -0.104111   -0.14399697  0.06177046  0.22053185 -0.03425767\n",
      "   0.15873055 -0.12941076  0.04503787  0.08094513]\n",
      " [ 0.08035401 -0.07040345  0.02121777  0.26021388  0.12603818  0.01224201\n",
      "   0.07452414 -0.13863645  0.02227862 -0.0519763 ]\n",
      " [ 0.23422952 -0.0156043   0.22610484  0.14163241  0.29596859 -0.19717015\n",
      "  -0.0510012  -0.06156255 -0.27850202  0.16506544]\n",
      " [ 0.15690048 -0.04094946  0.05574416  0.14326149  0.04848111 -0.07924569\n",
      "   0.19387235  0.13866799 -0.03811326 -0.02583651]\n",
      " [ 0.12316903 -0.06947979 -0.06495317  0.13371745  0.19325814 -0.03747279\n",
      "   0.19568501 -0.18096127  0.0482384   0.35217446]\n",
      " [ 0.15940329 -0.07972675 -0.14294007  0.28143242  0.30507013 -0.00109284\n",
      "   0.13875949 -0.08650678  0.00831238  0.17442736]\n",
      " [ 0.21896265 -0.00953498  0.11640885  0.16540484  0.1279815  -0.03038377\n",
      "   0.17885019 -0.14963888 -0.13736121  0.00336631]\n",
      " [ 0.26542392 -0.10922253  0.05306913  0.16828139  0.33073533  0.08265083\n",
      "   0.00842958 -0.26647019  0.00481534  0.08307828]\n",
      " [ 0.09741222 -0.10808437  0.06727661  0.22828175  0.07857981 -0.03278055\n",
      "   0.20661542 -0.2873157   0.05842017  0.09073374]\n",
      " [ 0.08701644  0.1374176  -0.13760963  0.09393863  0.237014   -0.06830726\n",
      "   0.12900051 -0.02554148 -0.03534425  0.13827376]\n",
      " [ 0.18858972 -0.21389858  0.08014549  0.29218411  0.22580297 -0.16665086\n",
      "   0.2293939   0.00168144  0.03473348  0.15763204]\n",
      " [ 0.19487059 -0.07009219  0.08151089  0.13212588  0.04883311  0.06196783\n",
      "   0.24787417 -0.23696594 -0.01591047  0.28816086]\n",
      " [ 0.05528795  0.18490414 -0.1267475   0.29646701  0.03950444 -0.14229061\n",
      "   0.06869195 -0.13259411  0.07701461 -0.05859036]\n",
      " [ 0.17340069 -0.12825805 -0.06008793  0.1298956   0.05747515 -0.14690781\n",
      "   0.18491878 -0.17987488  0.00286836  0.23950222]\n",
      " [ 0.2805393  -0.03817648  0.01187075  0.2483259   0.10263026 -0.05435124\n",
      "   0.10620125 -0.18766598 -0.14556913  0.04032617]\n",
      " [ 0.0696779  -0.1910415   0.06204317  0.1239261   0.02990784 -0.00720053\n",
      "   0.16153233 -0.133238   -0.06963314  0.12669739]\n",
      " [ 0.03920313  0.13189833 -0.03509643  0.12973809  0.07373457 -0.09076334\n",
      "   0.12195552  0.03077479 -0.15177885 -0.07153545]\n",
      " [ 0.04627009  0.07337394 -0.04493649  0.1588057   0.03123363 -0.06948993\n",
      "   0.18458758 -0.12711768 -0.13633233  0.04333826]\n",
      " [ 0.15115525 -0.02382171 -0.05987053  0.20653394  0.06033419  0.01999813\n",
      "   0.02250872 -0.17756508  0.01485377 -0.09859601]\n",
      " [ 0.12361348  0.27319667  0.01719444  0.12858245  0.07718511 -0.05663338\n",
      "   0.13934214 -0.06421265 -0.03480024 -0.12051915]\n",
      " [ 0.12960158  0.06391033  0.06726602  0.13208804  0.0859549   0.05788622\n",
      "   0.224701   -0.21065667 -0.16866557  0.07934968]\n",
      " [ 0.27473021  0.07593609  0.01581497  0.16828644  0.08555277 -0.086659\n",
      "  -0.01808003 -0.06836981 -0.1538772   0.03466695]\n",
      " [ 0.25516558 -0.0859812   0.12352162  0.07112218  0.25030079 -0.15308699\n",
      "   0.20225266 -0.07179128  0.14383046  0.16001569]\n",
      " [ 0.11887764  0.1596528  -0.16074947  0.04105639  0.17011918  0.02766917\n",
      "   0.1159912  -0.05761344 -0.01624066  0.05759097]\n",
      " [ 0.32515821  0.05442765 -0.01397619  0.22987562  0.1843686  -0.02572473\n",
      "   0.02770172 -0.26699105  0.10156634  0.27931255]\n",
      " [ 0.20952947  0.07351315  0.08077825  0.06285263  0.13091986 -0.03318001\n",
      "   0.07040979 -0.12240107  0.06975403  0.0421167 ]\n",
      " [ 0.3080743  -0.06732967  0.09814284  0.3118175   0.09849398 -0.21233876\n",
      "  -0.00389902 -0.39742443 -0.09360515  0.27498898]\n",
      " [ 0.13096169  0.07420453  0.08355839  0.13265148  0.12762071 -0.09221698\n",
      "   0.22955753 -0.18046917  0.04630674  0.01189685]\n",
      " [ 0.07466061  0.08246572 -0.05240398  0.12462346  0.05198966 -0.0655053\n",
      "   0.10749139 -0.1932071   0.0921478   0.04247867]\n",
      " [ 0.08178169  0.05148416 -0.09995339  0.06290455  0.07597992  0.01317741\n",
      "   0.13705926 -0.06859253  0.0933077  -0.00513094]\n",
      " [ 0.13059589 -0.0409723  -0.00394914  0.20935085  0.21779548  0.08894041\n",
      "   0.24185292 -0.02129342 -0.03941566  0.09293111]\n",
      " [ 0.13096014 -0.06429139 -0.0117854   0.26367411  0.10068028  0.0268039\n",
      "  -0.03630922 -0.2442816  -0.17763485  0.00699777]\n",
      " [ 0.14861643 -0.1577332  -0.05154654  0.16836309  0.06042742 -0.06926791\n",
      "   0.20627308 -0.09537295 -0.26230329  0.20568499]\n",
      " [ 0.13858932  0.14712733 -0.05185328  0.05301032  0.18397065 -0.00755147\n",
      "   0.04859087 -0.16606779 -0.11872289  0.12925896]\n",
      " [ 0.13943104  0.0493343  -0.12343332  0.27519891  0.05220803 -0.01015022\n",
      "   0.17442663 -0.30898741  0.0378731   0.00453743]\n",
      " [ 0.310729    0.1052013  -0.05471259  0.02210767 -0.00694681 -0.08120291\n",
      "   0.28100497 -0.08566014 -0.10115933  0.1240972 ]\n",
      " [ 0.11665063  0.15152124  0.04592312  0.14964123  0.07449745 -0.05050571\n",
      "   0.16320428  0.07141939 -0.08910575 -0.05248108]\n",
      " [ 0.0227344  -0.13023894 -0.30123442  0.24852973  0.21227853 -0.21720541\n",
      "   0.07760319 -0.21712805 -0.00386153  0.21903934]\n",
      " [ 0.1707443  -0.08052572 -0.22403944  0.28051707  0.07998157 -0.17879786\n",
      "   0.28480801 -0.28209671  0.04039494  0.02473886]\n",
      " [ 0.16040742 -0.07782346 -0.09760553  0.06450863  0.11286129  0.00634935\n",
      "   0.21018298 -0.03383833 -0.20048842  0.00042289]\n",
      " [-0.00335879  0.13566174 -0.04922746  0.19872862  0.00310173 -0.15388784\n",
      "   0.20573959  0.03511324  0.00205514  0.08452789]]\n",
      "2.28014588356\n"
     ]
    }
   ],
   "source": [
    "# predict without Classifier\n",
    "y = model.predictor(x)\n",
    "loss = F.softmax_cross_entropy(y, t)\n",
    "#print(y.data)\n",
    "print(loss.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.28014588356\n"
     ]
    }
   ],
   "source": [
    "# predict with Classifier\n",
    "loss = model(x, t)\n",
    "print(loss.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 簡単に時間計測"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The slowest run took 4.39 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
      "1000 loops, best of 3: 1.19 ms per loop\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "y = model.predictor(x)\n",
    "loss = F.softmax_cross_entropy(y, t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "多分Accuracyの計算が入っているかいないかで計算時間が変わる。   \n",
    "Flagでself.compute_accuracyをつけると良い。  \n",
    "https://github.com/pfnet/chainer/blob/v1.19.0/chainer/links/model/classifier.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 loops, best of 3: 1.41 ms per loop\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "loss = model(x, t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## update\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.cleargrads()\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -2.11363169e-03,  -1.38228748e-03,  -5.11461927e-04,\n",
       "        -4.56609094e-04,  -1.31238729e-03,  -1.66235387e-03,\n",
       "        -1.29003066e-03,  -9.21345490e-04,  -9.10570612e-04,\n",
       "        -7.74228130e-04,  -7.32739863e-04,  -4.95720014e-04,\n",
       "        -5.64320420e-04,  -2.39379238e-04,   1.68128259e-04,\n",
       "        -8.32440346e-05,  -2.68386299e-04,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00], dtype=float32)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predictor.l1.W.grad[0][400:420]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "optimizer.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 : 2.26911950111\n",
      "100 : 1.37029600143\n",
      "200 : 0.689813196659\n",
      "300 : 0.363195955753\n",
      "400 : 0.213089868426\n",
      "500 : 0.138499110937\n",
      "600 : 0.0976763814688\n",
      "700 : 0.0731864646077\n",
      "800 : 0.0573908016086\n",
      "900 : 0.0465756244957\n",
      "1000 : 0.038815908134\n",
      "1100 : 0.0330368168652\n",
      "1200 : 0.0285944696516\n",
      "1300 : 0.0250963009894\n",
      "1400 : 0.0222843922675\n",
      "1500 : 0.0199799295515\n",
      "1600 : 0.0180639214814\n",
      "1700 : 0.0164525602013\n",
      "1800 : 0.0150790829211\n",
      "1900 : 0.0138964653015\n"
     ]
    }
   ],
   "source": [
    "# repeat manually\n",
    "for i in range(2000):\n",
    "    loss = model(x, t)\n",
    "    if i % 100 is 0:\n",
    "        print(\"{} : {}\".format(i, loss.data))\n",
    "    model.cleargrads()\n",
    "    loss.backward()\n",
    "    optimizer.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run all\n",
    "GPU: 0  \n",
    "unit: 1000  \n",
    "Minibatch-size: 100  \n",
    "epoch: 20  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "from __future__ import print_function\n",
    "import argparse\n",
    "import sys; sys.argv=['']; del sys\n",
    "\n",
    "import numpy as np\n",
    "import chainer\n",
    "import chainer.functions as F\n",
    "import chainer.links as L\n",
    "from chainer import training\n",
    "from chainer.training import extensions\n",
    "from chainer import Variable\n",
    "import copy # test iterator copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class MLP(chainer.Chain):\n",
    "    def __init__(self, n_units, n_out):\n",
    "        super(MLP, self).__init__(\n",
    "             # the size of the inputs to each layer will be inferred\n",
    "            l1=L.Linear(784, n_units),  # n_in -> n_units\n",
    "            l2=L.Linear(n_units, n_units),  # n_units -> n_units\n",
    "            l3=L.Linear(n_units, n_out),    # n_units -> n_out\n",
    "            )\n",
    "    def __call__(self, x):\n",
    "        h1 = F.relu(self.l1(x))\n",
    "        h2 = F.relu(self.l2(h1))\n",
    "        y = self.l3(h2)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Classifier(chainer.Chain):\n",
    "    def __init__(self, predictor):\n",
    "        super(Classifier, self).__init__(predictor=predictor)\n",
    "        self.accuracy = None\n",
    "    def __call__(self, x, t):\n",
    "        y = self.predictor(x)\n",
    "        loss = F.softmax_cross_entropy(y, t)\n",
    "        self.accuracy = F.accuracy(y, t)\n",
    "        chainer.report({'loss': loss, 'accuracy': self.accuracy}, self)\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unitsize = 1000\n",
    "batchsize = 100\n",
    "epochsize = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gpu_id = 0 #  use gpu\n",
    "model = Classifier(MLP(unitsize, 10))\n",
    "\n",
    "if gpu_id >= 0:\n",
    "    chainer.cuda.get_device(gpu_id).use()  # Make a specified GPU current\n",
    "    model.to_gpu()  # Copy the model to the GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# optimizer\n",
    "# optimizer = chainer.optimizers.SGD()\n",
    "optimizer = chainer.optimizers.Adam()\n",
    "\n",
    "optimizer.use_cleargrads()\n",
    "optimizer.setup(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the MNIST dataset\n",
    "train, test = chainer.datasets.get_mnist()\n",
    "train_iter = chainer.iterators.SerialIterator(train, 100)\n",
    "test_iter = chainer.iterators.SerialIterator(test, 100, repeat=False, shuffle=False)\n",
    "testsize = len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 0, Mean loss: 0.103590451181, Mean accuracy: 0.965399980545\n",
      "epoch : 1, Mean loss: 0.0772355273366, Mean accuracy: 0.975199997425\n",
      "epoch : 2, Mean loss: 0.0724863857031, Mean accuracy: 0.977900028229\n",
      "epoch : 3, Mean loss: 0.0671059712768, Mean accuracy: 0.979600012302\n",
      "epoch : 4, Mean loss: 0.0751234218478, Mean accuracy: 0.978900015354\n",
      "epoch : 5, Mean loss: 0.0699494481087, Mean accuracy: 0.981100022793\n",
      "epoch : 6, Mean loss: 0.0859361812472, Mean accuracy: 0.978200018406\n",
      "epoch : 7, Mean loss: 0.086266040802, Mean accuracy: 0.978100001812\n",
      "epoch : 8, Mean loss: 0.0963259860873, Mean accuracy: 0.977999985218\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# run\n",
    "xp = model.xp\n",
    "\n",
    "pbar = tqdm(xrange(epochsize))\n",
    "for epoch in pbar:\n",
    "    for batch in train_iter:\n",
    "        # data separation\n",
    "        x = xp.asarray([it[0] for it in batch], dtype=np.float32)\n",
    "        t = xp.asarray([it[1] for it in batch], dtype=np.int32)\n",
    "        # compute grad\n",
    "        loss = model(x, t)\n",
    "        model.cleargrads()\n",
    "        loss.backward()\n",
    "        optimizer.update()\n",
    "\n",
    "        # terminate\n",
    "        if train_iter.is_new_epoch is True:\n",
    "            break\n",
    "\n",
    "    # evaluate model   \n",
    "    sum_loss = 0\n",
    "    sum_acc = 0\n",
    "    test_iter_copy = copy.copy(test_iter)\n",
    "    for test_batch in test_iter_copy:\n",
    "        # data separation\n",
    "        x = xp.asarray([it[0] for it in test_batch], dtype=np.float32)\n",
    "        t = xp.asarray([it[1] for it in test_batch], dtype=np.int32)\n",
    "        # compute grad\n",
    "        loss = model(x, t)\n",
    "        sum_loss += loss.data * len(test_batch)\n",
    "        sum_acc += model.accuracy.data * len(test_batch)\n",
    "    mean_loss = sum_loss / testsize\n",
    "    mean_acc = sum_acc / testsize\n",
    "#     print(''.format(epoch=epoch))\n",
    "    print('epoch : {epoch}, Mean loss: {loss}, Mean accuracy: {acc}'.format(epoch=epoch, loss=mean_loss, acc=mean_acc))\n",
    "    # pbar.set_description('epoch : {epoch}'.format(epoch=epoch))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "widgets": {
   "state": {
    "366ede9cd4e448eeaaad604af59ee0f1": {
     "views": [
      {
       "cell_index": 39
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DDPG example\n",
    "\n",
    "この例では，ChainerによるDDPGの実装と，OpenAI Gymへの適用を行います．DQNの例の説明を前提としています．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import random\n",
    "import collections\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import chainer\n",
    "from chainer import functions as F\n",
    "from chainer import links as L\n",
    "from chainer import optimizers\n",
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DQNと同様，DDPGでもニューラルネットによってQ関数を近似します．ただしDQNの場合と異なり，DDPGでは連続行動空間を扱います．そのため各行動の価値をそれぞれニューラルネットが出力するということはできず，代わりに状態表現と一緒に行動も入力に含めます．インタフェースは以下のようになります．\n",
    "\n",
    "- 入力: 観測 + 行動 (ndim_obs + ndim_action 次元のベクトル)\n",
    "- 出力: 価値 (スカラー)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class QFunction(chainer.Chain):\n",
    "\n",
    "    def __init__(self, ndim_obs, ndim_action, n_hidden_channels=100):\n",
    "        super(QFunction, self).__init__(\n",
    "            l0=L.Linear(ndim_obs + ndim_action, n_hidden_channels),\n",
    "            l1=L.Linear(n_hidden_channels, n_hidden_channels),\n",
    "            l2=L.Linear(n_hidden_channels, 1, wscale=1e-3))\n",
    "\n",
    "    def __call__(self, s, a):\n",
    "        x = F.concat((s, a), axis=1)\n",
    "        h = F.relu(self.l0(x))\n",
    "        h = F.relu(self.l1(h))\n",
    "        return self.l2(h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DDPGではQ関数だけでなく方策も同様にニューラルネットによって表現します．特にDDPGは決定的な方策，すなわち状態が決まれば行動も決定的に決まる方策を用います．インタフェースは次のようになります．\n",
    "\n",
    "- 入力: 観測 (ndim_obs次元のベクトル)\n",
    "- 出力: 行動 (ndim_action次元のベクトル)\n",
    "\n",
    "なお，多くのタスクでは行動がとれる値の範囲に制限があります．この例では，tanh関数でニューラルネットの出力を丸め，行動がその範囲に収まるようにしています．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def squash(x, high, low):\n",
    "    center = (high + low) / 2\n",
    "    scale = (high - low) / 2\n",
    "    return F.tanh(x) * scale + center\n",
    "\n",
    "\n",
    "class Policy(chainer.Chain):\n",
    "\n",
    "    def __init__(self, ndim_obs, ndim_action, action_low, action_high,\n",
    "                 n_hidden_channels=100):\n",
    "        self.action_high = action_high\n",
    "        self.action_low = action_low\n",
    "        super(Policy, self).__init__(\n",
    "            l0=L.Linear(ndim_obs, n_hidden_channels),\n",
    "            l1=L.Linear(n_hidden_channels, n_hidden_channels),\n",
    "            l2=L.Linear(n_hidden_channels, 1, wscale=1e-3))\n",
    "\n",
    "    def __call__(self, x):\n",
    "        h = F.relu(self.l0(x))\n",
    "        h = F.relu(self.l1(h))\n",
    "        return squash(self.l2(h),\n",
    "                                 self.xp.asarray(self.action_high),\n",
    "                                 self.xp.asarray(self.action_low))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上で定義した`Policy`を使って行動を1つ選ぶ関数`get_action`を実装しておきます．なお，`Policy`の出力は，それがGPU上にあるなら`cupy.ndarray`，CPU上にあるなら`numpy.ndarray`となりますが，OpenAI Gymの環境に渡す行動は`numpy.ndarray`でなければなりません．そのために，`chainer.cuda.to_cpu`という関数を使い常に`numpy.ndarray`を返すようにしておきます．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_action(policy, obs):\n",
    "    xp = policy.xp\n",
    "    obs = xp.expand_dims(xp.asarray(obs, dtype=np.float32), 0)\n",
    "    with chainer.no_backprop_mode():\n",
    "        a = policy(obs).data[0]\n",
    "    return chainer.cuda.to_cpu(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`QFunction`と`Policy`のパラメータを更新する関数`update`を実装します．DQNと同様，それぞれのモデルに対してターゲットモデルが存在し，`Optimizer`も別々に用意します．\n",
    "\n",
    "以下の式では方策を$\\mu(s)$で表します．`QFunction`の更新式は，\n",
    "\n",
    "$$Q(s,a) \\leftarrow r + \\gamma Q_{\\text{target}}(s',\\mu_\\text{target}(s'))$$\n",
    "\n",
    "となり，maxをとるのではなく方策$\\mu_\\text{target}$に従った行動の価値を目標値とするという点でDQNと異なります．\n",
    "\n",
    "一方，`Policy`の更新式は，\n",
    "\n",
    "$$\\text{maximize}_\\mu Q(s,\\mu(s))$$\n",
    "\n",
    "という形を取り，これは価値の符号を反転させたもの`-Q(s, policy(s))`を損失とみなせば損失最小化問題とみなすことができます．Chainerでこの損失について`backward`メソッドを呼ぶと，`Q`と`policy`それぞれのパラメータについて勾配が計算されますが，`opt_policy.update`によって更新されるのは`policy`のパラメータだけです．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def update(Q, target_Q, policy, target_policy, opt_Q, opt_policy,\n",
    "           samples, gamma=0.99):\n",
    "    n = len(samples)\n",
    "    xp = Q.xp\n",
    "    s = xp.asarray([sample[0] for sample in samples], dtype=np.float32)\n",
    "    a = xp.asarray([sample[1] for sample in samples], dtype=np.float32)\n",
    "    r = xp.asarray([sample[2] for sample in samples], dtype=np.float32)\n",
    "    done = xp.asarray([sample[3] for sample in samples], dtype=np.float32)\n",
    "    s_next = xp.asarray([sample[4] for sample in samples], dtype=np.float32)\n",
    "    # Update Q\n",
    "    y = F.reshape(Q(s, a), (n,))\n",
    "    with chainer.no_backprop_mode():\n",
    "        next_q = F.reshape(target_Q(s_next, target_policy(s_next)), (n,))\n",
    "        t = r + gamma * (1 - done) * next_q\n",
    "    loss = F.mean_squared_error(y, t)\n",
    "    Q.cleargrads()\n",
    "    loss.backward()\n",
    "    opt_Q.update()\n",
    "    # Update policy\n",
    "    loss = - F.sum(Q(s, policy(s))) / n\n",
    "    policy.cleargrads()\n",
    "    loss.backward()\n",
    "    opt_policy.update()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DQNのように一定周期でターゲットモデルを同期する代わりに，DDPGではターゲットモデルのパラメータを現在のモデルにゆっくり追随させていく方法が提案されています．次の`soft_copy_params`がこれを実装した関数です．Chainerでは`Link.params()`メソッドで特定の`Link`のパラメータを列挙することができるので，それを使ってターゲットモデルのパラメータを一定の割合$\\tau$だけ現在のモデルに近づけていきます．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def soft_copy_params(source, target, tau):\n",
    "    for s, t in zip(source.params(), target.params()):\n",
    "        t.data[:] += tau * (s.data - t.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ここまでで必要な関数の実装は終わりました．以下ではアルゴリズムの全体を記述していきます．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "env_name = 'Pendulum-v0'\n",
    "M = 1000\n",
    "replay_start_size = 500\n",
    "minibatch_size = 64\n",
    "gpu = 0  # gpu id (-1 to use cpu)\n",
    "tau = 1e-2  # degree of soft target update\n",
    "reward_scale = 1e-3\n",
    "\n",
    "# Initialize an environment\n",
    "env = gym.make(env_name)\n",
    "ndim_obs = env.observation_space.low.size\n",
    "ndim_action = env.action_space.low.size\n",
    "\n",
    "# Initialize variables\n",
    "D = collections.deque(maxlen=10 ** 6)\n",
    "Rs = []\n",
    "step = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`QFunction`と`Policy`のインスタンスをそれぞれ作成し，ターゲットモデルと`Optimizer`もそれぞれ用意します．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Initialize chainer models\n",
    "Q = QFunction(ndim_obs, ndim_action)\n",
    "policy = Policy(ndim_obs, ndim_action,\n",
    "                env.action_space.low, env.action_space.high)\n",
    "if gpu >= 0:\n",
    "    chainer.cuda.get_device(gpu).use()\n",
    "    Q.to_gpu(gpu)\n",
    "    policy.to_gpu(gpu)\n",
    "target_Q = copy.deepcopy(Q)\n",
    "target_policy = copy.deepcopy(policy)\n",
    "opt_Q = optimizers.Adam()\n",
    "opt_Q.setup(Q)\n",
    "opt_policy = optimizers.Adam(alpha=1e-4)\n",
    "opt_policy.setup(policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下がアルゴリズムの本体にあたります．環境とのインタラクションのループを回しつつ，経験をreplay memoryに蓄積していき，そこからのサンプルにより`Q`および`policy`を更新していきます．環境とのインタラクションの際には，行動にガウシアンノイズを加えることによってexplorationを行っています．\n",
    "\n",
    "可視化のために各エピソードごとのスコア（報酬の和）を記録しプロットおり，学習の進行度合いを確認することができます．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Initialize a figure\n",
    "fig, ax = plt.subplots(1,1)\n",
    "\n",
    "for episode in range(M):\n",
    "\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    R = 0.0\n",
    "    t = 0\n",
    "\n",
    "    while not done and t < env.spec.timestep_limit:\n",
    "\n",
    "        # Select an action\n",
    "        a = get_action(policy, obs) + np.random.normal(scale=0.4)\n",
    "\n",
    "        # Execute an action\n",
    "        new_obs, r, done, _ = env.step(a)\n",
    "        # env.render(mode='rgb_array')\n",
    "        # env.render()\n",
    "        R += r\n",
    "\n",
    "        # Store a transition\n",
    "        D.append((obs, a, r * reward_scale, done, new_obs))\n",
    "        obs = new_obs\n",
    "\n",
    "        # Sample a random minibatch of transitions\n",
    "        if len(D) >= replay_start_size:\n",
    "            samples = random.sample(D, minibatch_size)\n",
    "            update(Q, target_Q, policy, target_policy,\n",
    "                   opt_Q, opt_policy, samples)\n",
    "\n",
    "        # Soft update of target models\n",
    "        soft_copy_params(Q, target_Q, tau)\n",
    "        soft_copy_params(policy, target_policy, tau)\n",
    "\n",
    "        step += 1\n",
    "        t += 1\n",
    "\n",
    "    Rs.append(R)\n",
    "    average_R = np.mean(Rs[-100:])\n",
    "    print('episode: {} step: {} R:{} average_R:{}'.format(\n",
    "          episode, step, R, average_R))\n",
    "    ax.clear()\n",
    "    ax.plot(Rs)\n",
    "    fig.canvas.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}